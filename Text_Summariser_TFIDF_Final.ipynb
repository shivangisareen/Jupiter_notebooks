{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we taking time to learn a process that is already automated? The answer is that learning to do the calculations by hand will give us insight into how tf-idf really works. This insight is valuable. Instead of viewing tf-idf scores as some magical number our spreadsheet or computer program gives us, we'll be able to explain where that number comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math\n",
    "import operator\n",
    "import statistics\n",
    "from string import punctuation\n",
    "stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_file(fname):\n",
    "    \"\"\"\n",
    "    Get file from text doc\n",
    "    \"\"\"\n",
    "    f=open(fname,'r')\n",
    "    text=f.readlines()\n",
    "    text=''.join(text) #converting the list to type str\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_string_special_characters(s):\n",
    "    \"\"\"\n",
    "    This function removes special characters from within a string.\n",
    "    parameters: \n",
    "        s(str): single input string.\n",
    "    return: \n",
    "        stripped(str): A string with special characters removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace special character with ' '\n",
    "    stripped = re.sub('[^\\w\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "\n",
    "    # Change any whitespace to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "\n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    \n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"This function returns the \n",
    "    total number of words in the input text.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(text_sents_clean):\n",
    "    \"\"\"\n",
    "    this function splits the text into sentences and\n",
    "    considering each sentence as a document, calculates the \n",
    "    total word count of each.\n",
    "    \"\"\"\n",
    "    doc_info = []\n",
    "    i = 0\n",
    "    for sent in text_sents_clean:\n",
    "        i += 1 \n",
    "        count = count_words(sent)\n",
    "        temp = {'doc_id' : i, 'doc_length' : count}\n",
    "        doc_info.append(temp)\n",
    "    return doc_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_freq_dict(sents):\n",
    "    \"\"\"\n",
    "    This function creates a frequency dictionary\n",
    "    of each document that contains words other than\n",
    "    stop words.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    freqDict_list = []\n",
    "    for sent in sents:\n",
    "        i += 1\n",
    "        freq_dict = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word not in stop_words:\n",
    "                if word in freq_dict:\n",
    "                    freq_dict[word] += 1\n",
    "                else:\n",
    "                    freq_dict[word] = 1\n",
    "                temp = {'doc_id' : i, 'freq_dict': freq_dict}\n",
    "        freqDict_list.append(temp)\n",
    "    return freqDict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_frequency(text_sents_clean):\n",
    "    \"\"\"\n",
    "    This function returns a dictionary with the frequency \n",
    "    count of every word in the text\n",
    "    \"\"\"\n",
    "    freq_table = {}\n",
    "    text = ' '.join(text_sents_clean) #join the cleaned sentences to get the text \n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        word = ps.stem(word)\n",
    "        if word not in stop_words:\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "    return freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(text_sents_clean):\n",
    "    \"\"\"\n",
    "    This function gets the top 5 most\n",
    "    frequently occuring words in the whole text\n",
    "    and stores them as keywords\n",
    "    \"\"\"\n",
    "    freq_table = global_frequency(text_sents_clean)\n",
    "    #sort in descending order\n",
    "    freq_table_sorted = sorted(freq_table.items(), key = operator.itemgetter(1), reverse = True) \n",
    "    keywords = []\n",
    "    for i in range(0, 5):  #taking first 5 most frequent words\n",
    "        keywords.append(freq_table_sorted[i][0])\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(doc_info, freqDict_list):\n",
    "    \"\"\"\n",
    "    tf = (frequency of the term in the doc/total number of terms in the doc)\n",
    "    \"\"\"\n",
    "    TF_scores = []\n",
    "    \n",
    "    for tempDict in freqDict_list:\n",
    "        id = tempDict['doc_id']\n",
    "        for k in tempDict['freq_dict']:\n",
    "            temp = {'doc_id' : id,\n",
    "                    'TF_score' : tempDict['freq_dict'][k]/doc_info[id-1]['doc_length'],\n",
    "                   'key' : k}\n",
    "            TF_scores.append(temp)\n",
    "    return TF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(doc_info, freqDict_list):\n",
    "    \"\"\"\n",
    "    idf = ln(total number of docs/number of docs with term in it)\n",
    "    \"\"\"\n",
    "    \n",
    "    IDF_scores = []\n",
    "    counter = 0\n",
    "    for dict in freqDict_list:\n",
    "        counter += 1\n",
    "        for k in dict['freq_dict'].keys():\n",
    "            count = sum([k in tempDict['freq_dict'] for tempDict in freqDict_list])\n",
    "            temp = {'doc_id' : counter, 'IDF_score' : math.log(len(doc_info)/count), 'key' : k}\n",
    "    \n",
    "            IDF_scores.append(temp)\n",
    "                \n",
    "    return IDF_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before computing the word frequency, we must clean the data- remove punctuation \n",
    "and special characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_text_from_file('text.txt')\n",
    "text_sents = sent_tokenize(text)\n",
    "text_sents_clean = [remove_string_special_characters(s) for s in text_sents] #if s.istitle() == False]\n",
    "doc_info = get_doc(text_sents_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "freqDict_list = create_freq_dict(text_sents_clean)\n",
    "TF_scores = computeTF(doc_info, freqDict_list)\n",
    "IDF_scores = computeIDF(doc_info, freqDict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(TF_scores, IDF_scores):\n",
    "    \"\"\"\n",
    "    TFIDF is computed by multiplying the coressponding\n",
    "    TF and IDF values of each term. \n",
    "    \"\"\"\n",
    "    TFIDF_scores = []\n",
    "    for j in IDF_scores:\n",
    "        for i in TF_scores:\n",
    "            if j['key'] == i['key'] and j['doc_id'] == i['doc_id']:\n",
    "                temp = {'doc_id' : i['doc_id'],\n",
    "                        'TFIDF_score' : j['IDF_score']*i['TF_score'],\n",
    "                       'key' : i['key']}\n",
    "        TFIDF_scores.append(temp)\n",
    "    return TFIDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TFIDF_scores = computeTFIDF(TF_scores, IDF_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weigh_keywords(TFIDF_scores):\n",
    "    \"\"\"\n",
    "    This function doubles the TFIDF score\n",
    "    of the words that are keywords\n",
    "    \"\"\"\n",
    "    keywords = get_keywords(text_sents_clean)\n",
    "    for temp_dict in TFIDF_scores:\n",
    "        if temp_dict['key'] in keywords:\n",
    "            temp_dict['TFIDF_score'] *= 2\n",
    "    return TFIDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_scores = weigh_keywords(TFIDF_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_score(TFIDF_scores, text_sents, doc_info):\n",
    "    \"\"\"\n",
    "    This function prints out the summary and returns the \n",
    "    score of each sentence in a list.\n",
    "    \n",
    "    The score of a sentence is calculated by adding the TFIDF\n",
    "    scores of the words that make up the sentence.\n",
    "    \"\"\"\n",
    "    sentence_info = []\n",
    "    for doc in doc_info:\n",
    "        \"\"\"\n",
    "        This loops through each document(sentence)\n",
    "        and calculates their 'sent_score'\n",
    "        \"\"\"\n",
    "        sent_score = 0\n",
    "        for i in range(0, len(TFIDF_scores)):\n",
    "            temp_dict = TFIDF_scores[i]\n",
    "            if doc['doc_id'] == temp_dict['doc_id']:\n",
    "                sent_score += temp_dict['TFIDF_score']\n",
    "        temp = {'doc_id' : doc['doc_id'], 'sent_score' : sent_score,\n",
    "                'sentence' : text_sents[doc['doc_id']-1]}\n",
    "        sentence_info.append(temp)\n",
    "\n",
    "    return sentence_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentence_info = get_sent_score(TFIDF_scores, text_sents, doc_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(sentence_info):\n",
    "    sum = 0\n",
    "    summary = []\n",
    "    array = []\n",
    "    for temp_dict in sentence_info:\n",
    "        \"\"\"\n",
    "        This loop gets the sum of scores\n",
    "        of all the sentences.\n",
    "        \"\"\"\n",
    "        sum += temp_dict['sent_score']\n",
    "    avg = sum/len(sentence_info) #computing the average tf-idf score\n",
    "    for temp_dict in sentence_info:\n",
    "        \"\"\"\n",
    "        This loop gets the sentence scores \n",
    "        and stores them in an array.\n",
    "        \"\"\"\n",
    "        array.append(temp_dict['sent_score'])\n",
    "    stdev = statistics.stdev(array) #computing standard deviation on the array   \n",
    "    for sent in sentence_info:\n",
    "        \"\"\"\n",
    "        This loop is for getting the sumamry by \n",
    "        extracting sentences by an if clause\n",
    "        \"\"\"\n",
    "        if(sent['sent_score']) >= avg: # + 1.5*stdev:\n",
    "            summary.append(sent['sentence'])\n",
    "    summary = '\\n'.join(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An explosion at an industrial park in China's Sichuan province has left 19 people dead and 12 others injured, officials say.\n",
      "Photos on Chinese social media showed a huge fire and plumes of smoke rising from the facility.\n",
      "Authorities have begun an investigation, the news agency said.\n",
      "Yibin Hengda makes chemicals for the food and pharmaceutical industries.\n",
      "The accident follows previous high-profile disasters at chemical plants in China.\n",
      "An official investigation into the blast found that corruption, political connections and official collusion allowed corners to be cut and safety systems to be overridden.\n"
     ]
    }
   ],
   "source": [
    "summary = get_summary(sentence_info)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
