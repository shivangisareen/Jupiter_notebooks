{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk. corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "class FrequencySummarizer:\n",
    "    def __init__(self, min_cut=0.1, max_cut=0.9):\n",
    "        \"\"\"\n",
    "            Initialize the text summarizer.\n",
    "            words that have a frequency term lower than min_cut\n",
    "            and higher than max_cut will be ignored\n",
    "        \"\"\"\n",
    "        \n",
    "        self._min_cut=min_cut\n",
    "        self._max_cut=max_cut\n",
    "        self._stopwords=set(stopwords.words('english') + list(punctuation))\n",
    "        \n",
    "        \n",
    "    def _compute_freq(self, word_sent):\n",
    "        \"\"\"\n",
    "            Compute the frequency of each word.\n",
    "            Input:\n",
    "            word_sent, a list of all sentences already tokenized.\n",
    "            Output:\n",
    "            freq, a dictionary where freq[a] is the freq of a.\n",
    "        \"\"\"\n",
    "        \n",
    "        freq=defaultdict(int)\n",
    "        for s in word_sent:\n",
    "            for word in s:\n",
    "                if word not in self._stopwords:\n",
    "                    freq[word]+=1\n",
    "                    \n",
    "        #freq normalization and filtering\n",
    "        m=float(max(freq.values()))\n",
    "        for w in freq.keys():\n",
    "            if freq[w] >= self._max_cut or freq[w] <= self._min_cut:\n",
    "                del freq[w]\n",
    "        return freq\n",
    "    \n",
    "    \n",
    "    def summarize(self, text, n):\n",
    "        \"\"\"\n",
    "            Return a list of n sentences\n",
    "            which represent the summary of the text\n",
    "        \"\"\"\n",
    "        \n",
    "        sents=sent_tokenize(text)   #sent_tokenize already built in\n",
    "        assert n <= len(sents)     \n",
    "        word_sent=[word_tokenize(s.lower()) for s in sents]   #making the words lowercase\n",
    "        self._freq=self._compute_frequencies(word_sent)\n",
    "        ranking=defaultdict(int)\n",
    "        for i, sent in enumerate(word_sent):\n",
    "            for w in sent:\n",
    "                if w in self._freq:\n",
    "                    ranking[i] += self._freq[w]\n",
    "        sents_idx=self._rank(ranking, n)\n",
    "        return [sents[j] for j in sents_idx]\n",
    "    \n",
    "    def _rank(self, ranking, n):\n",
    "        \"\"\" will return the first n sentences with highest ranking \"\"\"\n",
    "        return nlargest(n, ranking, key=ranking.get)\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The FrequencySummarizer tokenizes the input into sentences then computes the term frequency. The frequency map is filtered to remove words whose frequency is lesser than the min threshold and words whose frequency is greater than the max threshold, that occur frequently but dont carry much information, such as determiners  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/shivangisareen/anaconda3/lib/python3.6/site-packages (from bs4)\n",
      "Building wheels for collected packages: bs4\n",
      "  Running setup.py bdist_wheel for bs4 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/shivangisareen/Library/Caches/pip/wheels/a0/b0/b2/4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#create a function that extract the natural language from a HTML page using BeautifulSoup\n",
    "\n",
    "!pip install bs4\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_only_text(url):\n",
    " \"\"\" \n",
    "  return the title and the text of the article\n",
    "  at the specified url\n",
    " \"\"\"\n",
    " page = urllib.request.urlopen(url).read().decode('utf8')\n",
    " soup = BeautifulSoup(page)\n",
    " text = ' '.join(map(lambda p: p.text, soup.find_all('p')))   #all text extracted from the <p> tage\n",
    " return soup.title.text, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivangisareen/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/shivangisareen/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'map' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-923a03dfa208>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrequencySummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0marticle_url\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_summarize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_only_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'----------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'map' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "feed_xml = urllib.request.urlopen('http://feeds.bbci.co.uk/news/rss.xml').read()\n",
    "feed = BeautifulSoup(feed_xml.decode('utf8'))\n",
    "to_summarize = map(lambda p: p.text, feed.find_all('guid'))\n",
    "\n",
    "fs = FrequencySummarizer()\n",
    "for article_url in to_summarize[:5]:\n",
    "  title, text = get_only_text(article_url)\n",
    "  print ('----------------------------------')\n",
    "  print (title)\n",
    "  for s in fs.summarize(text, 2):\n",
    "   print ('*',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
